{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>OCR Google Vision API</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"part_intro\">Introduction</a></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the world of computer vision, one is very popular, Object Character Recognition (OCR). This method detects characters in a picture and translate them into text. This notebook will shows you how to use the OCR develop by Google and save the result of the detection in a text file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Content</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Introduction](#part_intro)\n",
    "- [Packages](#part_packages)\n",
    "- [Parameters](#part_0)\n",
    "- [Convert PDF to JPEG](#part_1)\n",
    "- [Setup credentials](#part_2)\n",
    "- [API Vision request](#part_3)\n",
    "    - [Functions needed](#part_3_1)\n",
    "    - [Call OCR per page](#part_3_2)\n",
    "    - [Call OCR per document (stack de 10 pages)](#part_3_3)\n",
    "    - [Multiprocessing](#part_3_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install google-cloud\n",
    "#!pip install google-cloud-storage\n",
    "#!pip install google-cloud-pubsub\n",
    "#!pip install google-cloud-translate\n",
    "#!pip install google-cloud-vision\n",
    "#!pip install pdf2image\n",
    "#!pip install google-api-python-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"part_packages\">Packages</a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from pdf2image import convert_from_bytes\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import base64\n",
    "import json\n",
    "import os\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import io\n",
    "from PIL import Image\n",
    "from google.cloud import pubsub_v1\n",
    "from google.cloud import vision\n",
    "\n",
    "from google.oauth2 import service_account\n",
    "import googleapiclient.discovery\n",
    "\n",
    "tqdm().pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"part_0\">Parameters</a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME_INPUT_FOLDER = \"PDF FOLDER NAME\"\n",
    "NAME_OUTPUT_FOLDER= \"RESULT TEXTS FOLDER\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_jpeg         = False\n",
    "per_page          = False\n",
    "per_document      = True    # stack 10 pages into one call \n",
    "multi_proc        = False    # use multiprocessing to call the OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_pdf = glob.glob(NAME_INPUT_FOLDER+\"/*.pdf\") # stock the name of the pdf files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"part_1\">I - Convert PDF into JPEG</a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_jpeg:\n",
    "    \n",
    "    for i in list_pdf:\n",
    "        # convert the pdf into jpeg\n",
    "        pages = convert_from_path(i, 500)\n",
    "        \n",
    "        for page in tqdm(enumerate(pages)):\n",
    "            # save each page \n",
    "            page[1].save(NAME_OUTPUT_FOLDER+\"/\"+i.split('/')[-1].split('.')[0]+'_'+str(page[0])+'.jpg', 'JPEG') # keep the name of the document and add increment="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"part_2\">II - Set up credentials</a></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lines below show how to create credentials with a service account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCOPES = ['https://www.googleapis.com/auth/cloud-vision']\n",
    "SERVICE_ACCOUNT_FILE = \"PUT YOUR SERVICE ACCOUNT JSON FILE HERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "        SERVICE_ACCOUNT_FILE, scopes=SCOPES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"part_3\">III - API Vision request</a></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a id=\"part_3_1\">Functions needed</a></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_text_document(content, credentials):\n",
    "    \"\"\"\n",
    "    Function to call the API vision and return the text detected inside the image\n",
    "    @param content: (bytes) image in bytes \n",
    "    @param credentials: credentials of the service account to call the API \n",
    "    @return: the text detected inside the picture\n",
    "    \"\"\"\n",
    "    \n",
    "    client = vision.ImageAnnotatorClient(credentials=credentials)\n",
    "    #with io.open(uri, 'rb') as image_file:\n",
    "    #    content = image_file.read()\n",
    "    \n",
    "    # load the image in bytes \n",
    "    image = vision.types.Image(content=content)\n",
    "    # call the OCR and keep text annotation\n",
    "    response = client.text_detection(image=image)\n",
    "    \n",
    "    # The actual response for the first page of the input file.\n",
    "    breaks = vision.enums.TextAnnotation.DetectedBreak.BreakType\n",
    "    paragraphs = []\n",
    "    lines = []\n",
    "    # extract text by block of detection\n",
    "    for page in response.full_text_annotation.pages:\n",
    "        for block in page.blocks:\n",
    "            for paragraph in block.paragraphs:\n",
    "                para = \"\"\n",
    "                line = \"\"\n",
    "                for word in paragraph.words:\n",
    "                    for symbol in word.symbols:\n",
    "                        line += symbol.text\n",
    "                        if symbol.property.detected_break.type == breaks.SPACE:\n",
    "                            line += ' '\n",
    "                        if symbol.property.detected_break.type == breaks.EOL_SURE_SPACE:\n",
    "                            line += ' '\n",
    "                            lines.append(line)\n",
    "                            para += line\n",
    "                            line = ''\n",
    "                        if symbol.property.detected_break.type == breaks.LINE_BREAK:\n",
    "                            lines.append(line)\n",
    "                            para += line\n",
    "                            line = ''\n",
    "                paragraphs.append(para)\n",
    "\n",
    "    \n",
    "    return \"\\n\".join(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_grid(images, max_horiz=np.iinfo(int).max):\n",
    "    \n",
    "    n_images = len(images)\n",
    "    n_horiz = min(n_images, max_horiz)\n",
    "    h_sizes, v_sizes = [0] * n_horiz, [0] * (n_images // n_horiz)\n",
    "    for i, im in enumerate(images):\n",
    "        h, v = i % n_horiz, i // n_horiz\n",
    "        h_sizes[h] = max(h_sizes[h], im.size[0])\n",
    "        v_sizes[v] = max(v_sizes[v], im.size[1])\n",
    "    h_sizes, v_sizes = np.cumsum([0] + h_sizes), np.cumsum([0] + v_sizes)\n",
    "    im_grid = Image.new('RGB', (h_sizes[-1], v_sizes[-1]), color='white')\n",
    "    for i, im in enumerate(images):\n",
    "        im_grid.paste(im, (h_sizes[i % n_horiz], v_sizes[i // n_horiz]))\n",
    "    return im_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_file_ocr(path, cred=credentials):\n",
    "    '''\n",
    "    Function to concat 10 pages of the document and feed them to the OCR\n",
    "    @param path: (str) path of the pdf\n",
    "    @param cred: google credentials \n",
    "    '''\n",
    "    imgs = convert_from_bytes(open(path, 'rb').read(), fmt=\"jpeg\")\n",
    "    nb_pages = len(imgs)\n",
    "    nb_remaining_pages = nb_pages\n",
    "    ocr_step = 10\n",
    "    current_ocr_page_nb = 0\n",
    "    text = []\n",
    "    while nb_remaining_pages > 0:\n",
    "        if nb_remaining_pages > ocr_step:\n",
    "            ocr_range = range(current_ocr_page_nb, ocr_step + current_ocr_page_nb)\n",
    "            nb_remaining_pages -= ocr_step\n",
    "            current_ocr_page_nb += ocr_step\n",
    "        else:\n",
    "            ocr_range = range(current_ocr_page_nb, current_ocr_page_nb + nb_remaining_pages)\n",
    "            nb_remaining_pages = 0\n",
    "        # call ocr with range\n",
    "        im_grid = pil_grid(imgs[ocr_range.start:ocr_range.stop],1)\n",
    "        temp = BytesIO()\n",
    "        im_grid.save(temp, format='jpeg')\n",
    "        text.append(detect_text_document(temp.getvalue(), cred))\n",
    "    np.savetxt(NAME_OUTPUT_FOLDER+\"/\"+path.split('/')[-1].split('.')[0]+'.txt', text, fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a id=\"part_3_2\">Call OCR per page</a></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if per_page:\n",
    "    # call the API vision per page of the pdf\n",
    "    for i in tqdm(list_pdf):\n",
    "        # open the pdf and convert it into a PlImage format jpeg\n",
    "        call_ocr_save_txt(i, cred=credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_ocr_save_txt(path, cred=credentials):\n",
    "    '''\n",
    "    Function to feed the OCR with each page of the pdf convert into jpeg\n",
    "    @param path: (str) path of the pdf\n",
    "    @param cred: google credentials \n",
    "    '''\n",
    "    pages = convert_from_bytes(open(path, 'rb').read(), fmt=\"jpeg\") \n",
    "    text = []\n",
    "    # run on each page of the pdf \n",
    "    for page in pages:\n",
    "        # cast the jpeg into bytes \n",
    "        temp = BytesIO()\n",
    "        page.save(temp, format='jpeg')\n",
    "\n",
    "            # save the result of the OCR inside the variable text \n",
    "        text.append(detect_text_document(temp.getvalue(), cred))\n",
    "        # save the result into txt file \n",
    "    np.savetxt(NAME_OUTPUT_FILE+\"/\"+i.split('/')[1].split('.')[0]+'.txt', text, fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a id=\"part_3\">Call OCR per document (stack de 10 pages)</a></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if per_document:\n",
    "    # In order to save money when calling th API you could \n",
    "    # stack to 10 pages of the pdf in one call \n",
    "    for doc_pdf in tqdm(list_pdf):\n",
    "\n",
    "        # call the function which convert into jpeg, stack 10 images\n",
    "        # and call the API, save the output into txt file \n",
    "        concat_file_ocr(doc_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a id=\"part_3\">Multi-processing</a></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if multi_proc:\n",
    "    nb_threads = mp.cpu_count()\n",
    "    print(f\"The number of available CPU is {nb_threads}\")\n",
    "    \n",
    "    if per_page:\n",
    "        pool = mp.Pool(processes=nb_threads)    # create threads corresponding to the number specified\n",
    "        result = pool.map(call_ocr_save_txt, list_pdf) # map the function with part of the list for each thread\n",
    "        \n",
    "    if per_document:\n",
    "        pool = mp.Pool(processes=nb_threads) \n",
    "        result = pool.map(concat_file_ocr, list_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dask</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
